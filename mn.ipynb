{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homa', 'hotu', 'hontu', 'hosi', 'hoti', 'homi', 'hohi', 'hotha', 'honti']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3.10\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from functions import transliterate_xml\n",
    "from clean_text import text_cleaner\n",
    "\n",
    "sys.path.insert(1, '../utilities/')\n",
    "from timeis import timeis, yellow, line, tic, toc\n",
    "\n",
    "cst_dir = \"../Cst4/Xml/\"\n",
    "output_dir = \"output/\"\n",
    "\n",
    "file_list = [\n",
    "    \"s0201m.mul.xml\",\n",
    "    \"s0202m.mul.xml\",\n",
    "    \"s0203m.mul.xml\"\n",
    "]\n",
    "\n",
    "# create a list of all words in mn\n",
    "mn_df = pd.read_csv(\n",
    "\t\"../frequency maps/output/word count csvs/sutta_majjhima_mūla.csv\",\n",
    "\tsep=\"\\t\",\n",
    "\theader=None)\n",
    "mn_words_list = mn_df[0].to_list()\n",
    "\n",
    "# all inflections dict\n",
    "with open(\"../inflection generator/output/all inflections dict\", \"rb\") as p:\n",
    "\tall_inflection_dict = pickle.load(p)\n",
    "\n",
    "\n",
    "def find_inflections_with_no_eg1(word):\n",
    "\theadwords_list = []\n",
    "\tsearch_list = []\n",
    "\n",
    "\t# find which headwords the inflection could belong to\n",
    "\tfor headword in all_inflection_dict:\n",
    "\t\tif word in all_inflection_dict[headword][\"inflections\"]:\n",
    "\t\t\theadwords_list += [headword]\n",
    "\n",
    "\t# build a list of inflected forms to search for\n",
    "\tfor headword in headwords_list:\n",
    "\t\tif all_inflection_dict[headword][\"sutta1\"] == False:\n",
    "\t\t\tsearch_list += all_inflection_dict[headword][\"inflections\"]\n",
    "\treturn search_list\n",
    "\n",
    "\n",
    "for word in mn_words_list:\n",
    "\tsearch_list = find_inflections_with_no_eg1(word)\n",
    "\tif search_list != []:\n",
    "\t\tbreak\n",
    "\n",
    "print(search_list)\n",
    "\n",
    "for search_word in search_list:\n",
    "\tsearch_sentence = re.compile(\n",
    "\t\t\t\tf\"(\\\\. |\\\\! |\\\\? |^)(.[^.]*?{search_word}.*?($|\\\\. |\\\\! |\\\\? ))\")\n",
    "\n",
    "txt = open(\"output/temp/test.txt\", \"w\")\n",
    "\n",
    "sutta_counter = 0\n",
    "\n",
    "for file_name in file_list:\n",
    "\n",
    "\t# transliterate the xml files into roman and save\n",
    "\twith open(f\"{cst_dir}{file_name}\", \"r\", encoding=\"UTF-16\") as f:\n",
    "\t\txml = f.read()\n",
    "\n",
    "\txml = transliterate_xml(xml)\n",
    "\n",
    "\twith open(f\"../Cst4/xml roman/{file_name}\", \"w\") as w:\n",
    "\t\tw.write(xml)\n",
    "\n",
    "\t# make the soup\n",
    "\tsoup = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "\t# remove all the \"pb\" tags\n",
    "\tpbs = soup.find_all(\"pb\")\n",
    "\tfor pb in pbs:\n",
    "\t\tpb.decompose()\n",
    "\n",
    "\t# remove all the notes\n",
    "\tnotes = soup.find_all(\"note\")\n",
    "\tfor note in notes:\n",
    "\t\tnote.decompose()\n",
    "\n",
    "\t# remove all the para tags\n",
    "\tpns = soup.find_all(\"p\")\n",
    "\tfor pn in pns:\n",
    "\t\tdel pn[\"n\"]\n",
    "\n",
    "\t# remove all the hi tags\n",
    "\this = soup.find_all(\"hi\")\n",
    "\tfor hi in his:\n",
    "\t\thi.unwrap()\n",
    "\n",
    "\twith open(f\"output/temp/{file_name}.xml\", \"w\") as w:\n",
    "\t\tw.write(soup.prettify())\n",
    "\n",
    "\t# paṇṇāsa = head.rend book\n",
    "\t# vagga = head.chapter\n",
    "\t# sutta name = p.subhead(with number)\n",
    "\t# subtitle = p.subhead(no number)\n",
    "\n",
    "\tnikaya = soup.find_all(\"p\", rend=\"nikaya\")[0].string\n",
    "\tpannasa = soup.find_all(\"head\", rend=\"book\")[0].string\n",
    "\n",
    "\tdivs = soup.find_all(\"div\", type=\"vagga\")\n",
    "\n",
    "\tfor div in divs:\n",
    "\n",
    "\t\tif div.head[\"rend\"] == \"chapter\":\n",
    "\t\t\t\tvagga_name = div.head.string\n",
    "\n",
    "\t\tparas = div.children\n",
    "\t\t\n",
    "\t\tfor para in paras:\n",
    "\t\t\t\n",
    "\t\t\t# get sutta and subtitle name and numbers\n",
    "\t\t\tif \"subhead\" in str(para):\n",
    "\t\t\t\tif re.findall(\"^\\\\d\", para.string):\n",
    "\t\t\t\t\tsutta_counter += 1\n",
    "\t\t\t\t\tsutta_name = para.string\n",
    "\t\t\t\t\tsutta_name_clean = re.sub(\"^\\\\d*\\\\. \", \"\", sutta_name)\n",
    "\t\t\t\t\tsutta_no = sutta_counter\n",
    "\t\t\t\t\tsubtitle = \"\" # close subtitle on each new sutta\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsubtitle = para.string\n",
    "\t\t\t\n",
    "\t\t\t# get text and search it\n",
    "\t\t\ttext = para.get_text()\n",
    "\t\t\tresults = re.findall(search_sentence, text)\n",
    "\n",
    "\t\t\t# clean up results and print\n",
    "\t\t\ttxt = open(\"output/temp/test.txt\", \"a\")\n",
    "\t\t\tfor result in results:\n",
    "\t\t\t\tclean_result = text_cleaner(result[1])\n",
    "\t\t\t\ttxt.write(f\"MN {sutta_counter}\\t{sutta_name_clean}\")\n",
    "\t\t\t\tif subtitle != \"\":\n",
    "\t\t\t\t\ttxt.write(f\", {subtitle}\")\n",
    "\t\t\t\ttxt.write(f\"\\t{clean_result}\")\n",
    "\t\t\t\ttxt.write(f\"\\n({nikaya} {pannasa} {vagga_name} {sutta_name})\\n\\n\")\n",
    "\n",
    "\ttxt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
